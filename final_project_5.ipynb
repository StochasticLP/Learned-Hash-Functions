{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "520ff7a4-306d-4de7-bf4f-50837abc67e9",
   "metadata": {},
   "source": [
    "Load the Enron email dataset from Kaggle into a pandas dataframe. Kaggle has already expressed that this dataset has completely unique values and there are no missing values. Therefore no duplication removal is necessary (since HashMaps can only have unique keys) and no data imputation is necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85b80fc3-741e-4c3e-a5ac-e13c27ac40cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1b50bb0-7225-42e6-9b10-2c648057aeba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       file                                            message\n",
      "0     allen-p/_sent_mail/1.  Message-ID: <18782981.1075855378110.JavaMail.e...\n",
      "1    allen-p/_sent_mail/10.  Message-ID: <15464986.1075855378456.JavaMail.e...\n",
      "2   allen-p/_sent_mail/100.  Message-ID: <24216240.1075855687451.JavaMail.e...\n",
      "3  allen-p/_sent_mail/1000.  Message-ID: <13505866.1075863688222.JavaMail.e...\n",
      "4  allen-p/_sent_mail/1001.  Message-ID: <30922949.1075863688243.JavaMail.e...\n",
      "(460937, 2)\n",
      "file       object\n",
      "message    object\n",
      "dtype: object\n",
      "0        allen-p/_sent_mail/1.\n",
      "1       allen-p/_sent_mail/10.\n",
      "2      allen-p/_sent_mail/100.\n",
      "3     allen-p/_sent_mail/1000.\n",
      "4     allen-p/_sent_mail/1001.\n",
      "5     allen-p/_sent_mail/1002.\n",
      "6     allen-p/_sent_mail/1003.\n",
      "7     allen-p/_sent_mail/1004.\n",
      "8      allen-p/_sent_mail/101.\n",
      "9      allen-p/_sent_mail/102.\n",
      "10     allen-p/_sent_mail/103.\n",
      "11     allen-p/_sent_mail/104.\n",
      "12     allen-p/_sent_mail/105.\n",
      "13     allen-p/_sent_mail/106.\n",
      "14     allen-p/_sent_mail/107.\n",
      "15     allen-p/_sent_mail/108.\n",
      "16     allen-p/_sent_mail/109.\n",
      "17      allen-p/_sent_mail/11.\n",
      "18     allen-p/_sent_mail/110.\n",
      "19     allen-p/_sent_mail/111.\n",
      "Name: file, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Clean the data\n",
    "csv.field_size_limit(100000000)\n",
    "emails_df = pd.read_csv('emails.csv', engine='python', on_bad_lines='skip')\n",
    "\n",
    "print(emails_df.head())\n",
    "print(emails_df.shape)\n",
    "print(emails_df.dtypes)\n",
    "\n",
    "# We really only care about the key data being clean here since the value data is irrelevant to training models. \n",
    "print(emails_df['file'].head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053315ff-1094-428a-95b3-0ed8fdc70f3d",
   "metadata": {},
   "source": [
    "Implementation of a traditional HashMap, class basis is from \"Geeks for Geeks\" (https://www.geeksforgeeks.org/hash-map-in-python/). Utilizes separate chaining to resolve collisions. Size indicates the capacity of the hashmap, the 'keys' parameter is an array of keys, and the 'vals' parameter is an array of the values to insert with the keys. This class assumes that the keys and values arrays are of the same size. This class has functionality to insert values into the hashmap, get values out of the hashmap using the key, and return the number of total conflicts while inserting pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f22bec16-ab67-4783-99c8-e1dea7ba9ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HashMap:\n",
    " \n",
    "    # Create empty bin list of given size\n",
    "    def __init__(self, size, keys, vals):\n",
    "        self.size = size\n",
    "        self.hash_table = self.create_bins()\n",
    "        self.keys = keys\n",
    "        self.vals = vals\n",
    "        self.conflict_count = 0\n",
    "\n",
    "        # Allows for initialization of the HashMap K/V pairs during its construction.\n",
    "        for i in range(0, len(keys)):\n",
    "            self.insert(keys[i], vals[i])\n",
    " \n",
    "    def create_bins(self):\n",
    "        return [[] for _ in range(self.size)]\n",
    " \n",
    "    # Insert values into hash map\n",
    "    def insert(self, key, val):\n",
    "       \n",
    "        # Get the index from the key\n",
    "        # using hash function\n",
    "        hashed_key = hash(key) % self.size\n",
    "         \n",
    "        # Get the bin corresponding to index\n",
    "        bin = self.hash_table[hashed_key]\n",
    " \n",
    "        found_key = False\n",
    "        for index, pair in enumerate(bin):\n",
    "            pair_key, pair_val = pair\n",
    "             \n",
    "            # check if the bin has same key as\n",
    "            # the key to be inserted\n",
    "            if pair_key == key:\n",
    "                found_key = True\n",
    "                break\n",
    "\n",
    "            self.conflict_count += 1\n",
    " \n",
    "        # If the bin has same key as the key to be inserted,\n",
    "        # Update the key value\n",
    "        # Otherwise append the new key-value pair to the bin\n",
    "        if found_key:\n",
    "            bin[index] = (key, val)\n",
    "        else:\n",
    "            bin.append((key, val))\n",
    " \n",
    "    # Return searched value with specific key\n",
    "    def get(self, key):\n",
    "       \n",
    "        # Get the index from the key using\n",
    "        # hash function\n",
    "        hashed_key = hash(key) % self.size\n",
    "         \n",
    "        # Get the bin corresponding to index\n",
    "        bin = self.hash_table[hashed_key]\n",
    " \n",
    "        found_key = False\n",
    "        for index, pair in enumerate(bin):\n",
    "            pair_key, pair_val = pair\n",
    "             \n",
    "            # check if the bucket has same key as \n",
    "            # the key being searched\n",
    "            if pair_key == key:\n",
    "                found_key = True\n",
    "                break\n",
    " \n",
    "        # If the bin has same key as the key being searched,\n",
    "        # Return the value found\n",
    "        # Otherwise indicate there was no record found\n",
    "        if found_key:\n",
    "            return pair_val\n",
    "        else:\n",
    "            return \"No pair found\"\n",
    "\n",
    "    # Get the number of conflicts after insertions are completed.\n",
    "    def get_conflicts(self):\n",
    "        return self.conflict_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f11a2f-362c-42fb-b94d-496b85501e36",
   "metadata": {},
   "source": [
    "Initialize a traditional hashmap to take in all the K/V pairs from the emails_df dataset with the 'file' column representing the keys and its corresponding 'message' column representing the values. The size of the HashMap is set to 100% the number of K/V pairs, so ideally with a perfect HashMap, there would be 0 collisions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "170a1035-e733-4c6f-b457-d4567bb24e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = []\n",
    "vals = []\n",
    "\n",
    "for index, row in emails_df.iterrows():\n",
    "    keys.append(row['file'])\n",
    "    vals.append(row['message'])\n",
    "\n",
    "hash_map = HashMap(len(keys), keys, vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c863fe-368c-4599-8663-8fcaebb3b3db",
   "metadata": {},
   "source": [
    "Now check the number of collisions that occurred with the traditional HashMap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "07649ef7-1505-43ce-94f7-2b664bbc1f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Hash Collisions: 230729\n",
      "Average number of collisions per key: 0.5005651531554204\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of Hash Collisions: {hash_map.get_conflicts()}')\n",
    "print(f'Average number of collisions per key: {hash_map.get_conflicts() / len(keys)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab95750a-d304-4bbd-9f52-4e816a6fdfe1",
   "metadata": {},
   "source": [
    "This is a notable number of collisions. How can we modify this to result in less hash collisions? \n",
    "Introducing, the learned hash model. With the learned hash model, the index structure exhibited in the traditional hash map will be decomposed into a CDF model in which we can find the position of the desired key in the backing array."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2a3d3e-3e0d-4553-ab45-d8a878d32349",
   "metadata": {},
   "source": [
    "Below is the RMINode class. This class takes in a subset of keys from the overall keys in the hashmap and creates a normalized CDF of the keys. A model is then trained on this CDF given the keys and will give the location of a key in the backing array. The CDF is used to utilize space most efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc7560a3-5bc8-49e1-b64d-ecc2c73fbbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take in offset through binary string, left movements are a 0, right movements are a 1. Left movements are dictated on if the\n",
    "# probability of a desired key with the specific CDF of 'this' RMINode <= 50, otherwise it is a right movement.\n",
    "\n",
    "class RMINode:\n",
    "\n",
    "    # Initialize an RMINode\n",
    "    def __init__(self, keys, isNormal, maxMin):\n",
    "        self.keys = keys\n",
    "        self.model = self.generate_model(keys, isNormal, maxMin)\n",
    "\n",
    "    # Generates a model using linear regression following either a normal distribution or uniform distribution.\n",
    "    def generate_model(self, keys, isNormal, maxMin):\n",
    "        feature_keys = np.array(keys).reshape(-1, 1)\n",
    "\n",
    "        if isNormal:\n",
    "            cdf = np.cumsum(keys) / np.sum(keys)\n",
    "        else:\n",
    "            cdf = uniform.cdf(keys, maxMin[1], maxMin[0])\n",
    "\n",
    "\n",
    "        model = LinearRegression()\n",
    "        model.fit(feature_keys, cdf)\n",
    "\n",
    "        return model\n",
    "\n",
    "    # Returns an array with two elements. The first element is the smaller half of the keys and the second element is the larger half of the keys.\n",
    "    def split_key_set(self):\n",
    "        return np.array_split(self.keys, 2)\n",
    "\n",
    "    # Returns the predicted probability of a key\n",
    "    def predict_key_probability(self, key):\n",
    "        prediction = self.model.predict([[key]])[0]\n",
    "\n",
    "        # unfortunately in application even with overfitting the data (which is beneficial for this model) there are still\n",
    "        # instances of negative or >1 probabilities being returned which is not in the feasible range of predictions.\n",
    "        if prediction > 1:\n",
    "            return 1\n",
    "        elif prediction < 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a930c5a-4c4a-4e9e-be2a-5ea448507c44",
   "metadata": {},
   "source": [
    "Below is the RMI class which stands for \"Recursive Model Index\". An RMI models a full tree, meaning that every level of the tree is filled up to depth k. This has a few nice properties which can be utilized for the learned hash function. Firstly a full tree can be modeled through an array rather than an actual tree structure which helps keep things efficient. Secondly, the number of nodes in a full tree can always be calculated through 2^(k+1) - 1 where k is the depth of the tree. It can be observed that this is exponential. As such, more depth to the full tree allows for greater precision but consumes much more cost computationally. k should be kept relatively low to keep the hash lookups in constant, O(1), time. Another useful property of full trees is that the number of nodes on the deepest (leaf) level is 2^k. \n",
    "\n",
    "RMI uses the full tree properties to instaniate a backing array of size 2^(k+1) - 1 and fills it with RMINodes (see documentation above). These RMINodes each contain a subset of the sorted keys and predict a key's position in its CDF given the key value. Once the leaf level is reached, RMI accounts for the offset from left-to-right of the sorted length(keys)/(2^k) partitions. Using this, it narrows down the position of a key in the backing array and returns it as the index to search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e00457aa-d1e5-49a3-af2e-050b4f1c9462",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMI:\n",
    "\n",
    "    def __init__(self, keys, k, isNormal):\n",
    "        self.keys = np.sort(np.array(keys))\n",
    "        self.k = k\n",
    "        self.isNormal = isNormal\n",
    "        self.hash_table = self.create_hash_table()\n",
    "\n",
    "        # This is the leaf level\n",
    "        self.smallest_partition_size = len(keys) / (2 ** k)\n",
    "        self.first_leaf_index = len(self.hash_table) - (2 << (self.k-1))\n",
    "\n",
    "    # Using the depth k given, create a full tree of RMINodes\n",
    "    def create_hash_table(self):\n",
    "        hash_table = [None] * (2 ** (self.k + 1) - 1)\n",
    "        self.fill_hash_table_recursive(hash_table, 0, self.keys)\n",
    "        return hash_table\n",
    "\n",
    "    # Recursively adds RMINodes to the full tree, splitting the dataset in half for each child node. \n",
    "    def fill_hash_table_recursive(self, hash_table, index, subkeys):\n",
    "\n",
    "        # Base case\n",
    "        if index >= len(hash_table):\n",
    "            return\n",
    "\n",
    "        hash_table[index] = RMINode(subkeys, self.isNormal, (subkeys[len(subkeys)-1], subkeys[0]))\n",
    "\n",
    "        # Fill the left and right children of this node, in a full tree (which the RMI is) the left child can be accessed from 2 * parent_index + 1,\n",
    "        # and the right child can be accessed from 2 * parent_index + 2\n",
    "\n",
    "        subset_keys = hash_table[index].split_key_set()\n",
    "        self.fill_hash_table_recursive(hash_table, 2 * index + 1, subset_keys[0])\n",
    "        self.fill_hash_table_recursive(hash_table, 2 * index + 2, subset_keys[1])\n",
    "\n",
    "    # Starting at the root RMINode, check the key's probability from the CDF and if it's less than or equal to 50%, go to the left child node,\n",
    "    # else go to the right child node and keep track of changes through a binary string such that left movements are a '0' and right movements\n",
    "    # are a '1'. \n",
    "    def get_hash_index(self, key):\n",
    "        cur_index = 0\n",
    "        key_probability = 0\n",
    "        key_subset_offset = 0\n",
    "\n",
    "        # Keep narrowing down the search until the leaf nodes are reached.\n",
    "        while cur_index < self.first_leaf_index:\n",
    "            key_probability = self.hash_table[cur_index].predict_key_probability(key)\n",
    "\n",
    "            if key_probability <= 0.5:\n",
    "                # Bitshift to the left one\n",
    "                key_subset_offset = key_subset_offset << 1\n",
    "                cur_index = 2 * cur_index + 1\n",
    "            else:\n",
    "                # Bitshift to the left one and bitwise OR a 1\n",
    "                key_subset_offset = (key_subset_offset << 1) | 1\n",
    "                cur_index = 2 * cur_index + 2\n",
    "\n",
    "        return round((key_probability + key_subset_offset) * self.smallest_partition_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d4ebb9-a84a-4646-acf2-8fe4073a0933",
   "metadata": {},
   "source": [
    "Now the RMI class will be used as the basis for the Learned Hash Map. The Learned Hash Map has the same functionality as the traditional hashmap but some additional parameters must be specified. The size parameter specifies the capacity of the hashmap, the keys parameter is an array of keys, and the values parameter is an array of values of the same size. Then, a value 'k' must be given which is the depth of of the internal RMI. Details on the implications of depth 'k' are given in the RMI class documentation. One important note is that your value of k will have 2^k leaf nodes in the RMI. As such you must have at least as many values to insert as the number of leaf nodes plus one. Secondly the value for k must be at least 2 to get actual benefit out of it. Hence, technically you can not make a hashmap smaller than size 5, but this class in general considers large datasets. Another parameter is the 'isNormal' which specifies whether the data should be approximated to a normal distribution (if true) or a uniform distribution (if false). Finally, the 'isText' parameter specifies whether text data is being used for the keys or not. Random variables map an item in the sample space to a number in the real number range. We couldn't find a good way to do this while still being O(1) time, so we had to use an internal dictionary to store which string key maps to which encoding. This is only used if isText is true, if the keys are numerical then this should be false. The hashmap currently has the functionality to insert values, get values from it, and get the total number of conflicts with insertions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fa4604f8-14db-4e6e-a307-eb96469e09a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearnedHashMap:\n",
    "\n",
    "    # Create empty bin list of given size\n",
    "    def __init__(self, size, keys, vals, k, isNormal, isText):\n",
    "        if k < 2:\n",
    "            raise Exception('You must at least have a depth of 2!')\n",
    "        if len(keys) < ((2 ** k) + 1):\n",
    "            raise Exception('You must have at least as many total values plus one to have one element per leaf of the RMI! ' + \n",
    "                            f'You must have at least {((2 ** k) + 1)} values to use this value of k!')\n",
    "        \n",
    "        self.size = size\n",
    "        self.hash_table = self.create_bins()\n",
    "        self.keys = keys\n",
    "        self.vals = vals\n",
    "        self.conflict_count = 0\n",
    "        self.isText = isText\n",
    "        self.keyToValue = {}\n",
    "        self.availableEncoder = 0\n",
    "\n",
    "        # Initialize the RMI with the keyset. If there is text data then assign unique label encoders for each key.\n",
    "        if self.isText:\n",
    "            encoders = []\n",
    "            for i in range (0, len(keys)):\n",
    "                encoders.append(i)\n",
    "            self.RMI = RMI(encoders, k, isNormal)\n",
    "        else:\n",
    "            self.RMI = RMI(keys, k, isNormal)\n",
    "\n",
    "        # Allows for initialization of the HashMap K/V pairs during its construction.\n",
    "        for i in range(0, len(keys)):\n",
    "            self.insert(keys[i], vals[i])\n",
    "\n",
    "    def create_bins(self):\n",
    "        return [[] for _ in range(self.size)]\n",
    "\n",
    "    # Insert values into hash map\n",
    "    def insert(self, key, val):\n",
    "\n",
    "        if self.isText:\n",
    "            if self.keyToValue.get(key) == None:\n",
    "                self.keyToValue[key] = self.availableEncoder\n",
    "                key = self.availableEncoder\n",
    "                self.availableEncoder += 1\n",
    "            else:\n",
    "                key = self.keyToValue[key]\n",
    "\n",
    "        # Get the index from the key\n",
    "        # using hash function\n",
    "        hashed_key = self.RMI.get_hash_index(key)\n",
    "\n",
    "        if hashed_key > len(self.keys) - 1:\n",
    "            hashed_key = len(self.keys) - 1\n",
    "\n",
    "        # Get the bin corresponding to index\n",
    "        bin = self.hash_table[hashed_key]\n",
    "\n",
    "        found_key = False\n",
    "        for index, pair in enumerate(bin):\n",
    "            pair_key, pair_val = pair\n",
    "\n",
    "            # check if the bin has same key as\n",
    "            # the key to be inserted\n",
    "            if pair_key == key:\n",
    "                found_key = True\n",
    "                break\n",
    "\n",
    "            self.conflict_count += 1\n",
    "\n",
    "        # If the bin has same key as the key to be inserted,\n",
    "        # Update the key value\n",
    "        # Otherwise append the new key-value pair to the bin\n",
    "        if found_key:\n",
    "            bin[index] = (key, val)\n",
    "        else:\n",
    "            bin.append((key, val))\n",
    "\n",
    "    # Return searched value with specific key\n",
    "    def get(self, key):\n",
    "\n",
    "        if self.isText:\n",
    "            key = self.keyToValue.get(key)\n",
    "            if key == None:\n",
    "                return \"No value found\"\n",
    "\n",
    "        # Get the index from the key using\n",
    "        # hash function\n",
    "        hashed_key = self.RMI.get_hash_index(key)\n",
    "\n",
    "        if hashed_key > len(self.keys) - 1:\n",
    "            hashed_key = len(self.keys) - 1\n",
    "\n",
    "        # Get the bin corresponding to index\n",
    "        bin = self.hash_table[hashed_key]\n",
    "\n",
    "        found_key = False\n",
    "        for index, pair in enumerate(bin):\n",
    "            pair_key, pair_val = pair\n",
    "\n",
    "            # check if the bucket has same key as\n",
    "            # the key being searched\n",
    "            if pair_key == key:\n",
    "                found_key = True\n",
    "                break\n",
    "\n",
    "        # If the bin has same key as the key being searched,\n",
    "        # Return the value found\n",
    "        # Otherwise indicate there was no record found\n",
    "        if found_key:\n",
    "            return pair_val\n",
    "        else:\n",
    "            return \"No value found\"\n",
    "\n",
    "    # Get the number of conflicts after insertions are completed.\n",
    "    def get_conflicts(self):\n",
    "        return self.conflict_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0016397e-db17-44a9-9d36-14739dad3650",
   "metadata": {},
   "source": [
    "Now, using the learned hash function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1af2fc4a-bf21-4ac2-9326-92a74e68193a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "806618\n"
     ]
    }
   ],
   "source": [
    "# Load in the enron email dataset with an RMI depth of 3. Since these keys are all unique a uniform distribution is a better approximation\n",
    "# than a normal distribution. The isText parameter is true since the keys are strings.\n",
    "hash_table_l = LearnedHashMap(len(keys), keys, vals, 3, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4bc55caa-8281-4b83-ad3c-bdbdc2c7c40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Hash Collisions: 806618\n",
      "Average number of collisions per key: 1.7499528135081366\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of Hash Collisions: {hash_table_l.get_conflicts()}')\n",
    "print(f'Average number of collisions per key: {hash_table_l.get_conflicts() / len(keys)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e0b8a7-bf4e-4015-99b0-da3b0288836f",
   "metadata": {},
   "source": [
    "With the learned hashmap, we surprisingly see more more conflicts on average \"1.7499528135081366\" as compared to the traditional hashmap with\n",
    "0.5005651531554204. This is good to know but may not be a definitive factor. Before we go on, as seen below, both hash maps do work properly and return the same value when the same key is inputted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0af330d9-dc48-4161-a059-2ee6c48d46fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allen-p/_sent_mail/14.\n",
      "Message-ID: <27936946.1075855378542.JavaMail.evans@thyme>\n",
      "Date: Wed, 2 May 2001 10:27:00 -0700 (PDT)\n",
      "From: phillip.allen@enron.com\n",
      "To: tori.kuykendall@enron.com\n",
      "Subject: Re: 2- SURVEY - PHILLIP ALLEN\n",
      "Mime-Version: 1.0\n",
      "Content-Type: text/plain; charset=us-ascii\n",
      "Content-Transfer-Encoding: 7bit\n",
      "X-From: Phillip K Allen\n",
      "X-To: Tori Kuykendall <Tori Kuykendall/HOU/ECT@ECT>\n",
      "X-cc: \n",
      "X-bcc: \n",
      "X-Folder: \\Phillip_Allen_Jan2002_1\\Allen, Phillip K.\\'Sent Mail\n",
      "X-Origin: Allen-P\n",
      "X-FileName: pallen (Non-Privileged).pst\n",
      "\n",
      "\n",
      "---------------------- Forwarded by Phillip K Allen/HOU/ECT on 05/02/2001 05:26 AM ---------------------------\n",
      "\n",
      "\n",
      "Ina Rangel\n",
      "05/01/2001 12:24 PM\n",
      "To:\tPhillip K Allen/HOU/ECT@ECT\n",
      "cc:\t \n",
      "Subject:\tRe: 2- SURVEY - PHILLIP ALLEN   \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "   \n",
      "-\n",
      "Full Name:        Phillip Allen\n",
      "\n",
      "Login ID:  \tpallen\n",
      "\n",
      "Extension:  3-7041\n",
      "\n",
      "Office Location:  EB3210C\n",
      "\n",
      "What type of computer do you have?  (Desktop,  Laptop,  Both)  Both\n",
      "\n",
      "Do you have a PDA?  If yes, what type do you have:   (None, IPAQ, Palm Pilot, Jornada)  IPAQ\n",
      "\n",
      "Do you have permission to access anyone's Email/Calendar?  NO\n",
      "    If yes, who?  \n",
      "\n",
      "Does anyone have permission to access your Email/Calendar?  YES\n",
      "    If yes, who?  INA RANGEL\n",
      "\n",
      "Are you responsible for updating anyone else's address book?  \n",
      "    If yes, who?  NO\n",
      "\n",
      "Is anyone else responsible for updating your address book?  \n",
      "    If yes, who?  NO\n",
      "\n",
      "Do you have access to a shared calendar?  \n",
      "    If yes, which shared calendar?  YES, West Calendar\n",
      "\n",
      "Do you have any Distribution Groups that Messaging maintains for you (for mass mailings)?  \n",
      "    If yes, please list here:  NO\n",
      "\n",
      "Please list all Notes databases applications that you currently use:  NONE\n",
      "\n",
      "In our efforts to plan the exact date/time of your migration, we also will need to know:\n",
      "\n",
      "What are your normal work hours?  From:     7:00 AM To:      5:00 PM\n",
      "\n",
      "Will you be out of the office in the near future for vacation, leave, etc? NO\n",
      "       If so, when?        From (MM/DD/YY):      To (MM/DD/YY):      \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<Embedded StdOleLink>\n",
      "Message-ID: <27936946.1075855378542.JavaMail.evans@thyme>\n",
      "Date: Wed, 2 May 2001 10:27:00 -0700 (PDT)\n",
      "From: phillip.allen@enron.com\n",
      "To: tori.kuykendall@enron.com\n",
      "Subject: Re: 2- SURVEY - PHILLIP ALLEN\n",
      "Mime-Version: 1.0\n",
      "Content-Type: text/plain; charset=us-ascii\n",
      "Content-Transfer-Encoding: 7bit\n",
      "X-From: Phillip K Allen\n",
      "X-To: Tori Kuykendall <Tori Kuykendall/HOU/ECT@ECT>\n",
      "X-cc: \n",
      "X-bcc: \n",
      "X-Folder: \\Phillip_Allen_Jan2002_1\\Allen, Phillip K.\\'Sent Mail\n",
      "X-Origin: Allen-P\n",
      "X-FileName: pallen (Non-Privileged).pst\n",
      "\n",
      "\n",
      "---------------------- Forwarded by Phillip K Allen/HOU/ECT on 05/02/2001 05:26 AM ---------------------------\n",
      "\n",
      "\n",
      "Ina Rangel\n",
      "05/01/2001 12:24 PM\n",
      "To:\tPhillip K Allen/HOU/ECT@ECT\n",
      "cc:\t \n",
      "Subject:\tRe: 2- SURVEY - PHILLIP ALLEN   \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "   \n",
      "-\n",
      "Full Name:        Phillip Allen\n",
      "\n",
      "Login ID:  \tpallen\n",
      "\n",
      "Extension:  3-7041\n",
      "\n",
      "Office Location:  EB3210C\n",
      "\n",
      "What type of computer do you have?  (Desktop,  Laptop,  Both)  Both\n",
      "\n",
      "Do you have a PDA?  If yes, what type do you have:   (None, IPAQ, Palm Pilot, Jornada)  IPAQ\n",
      "\n",
      "Do you have permission to access anyone's Email/Calendar?  NO\n",
      "    If yes, who?  \n",
      "\n",
      "Does anyone have permission to access your Email/Calendar?  YES\n",
      "    If yes, who?  INA RANGEL\n",
      "\n",
      "Are you responsible for updating anyone else's address book?  \n",
      "    If yes, who?  NO\n",
      "\n",
      "Is anyone else responsible for updating your address book?  \n",
      "    If yes, who?  NO\n",
      "\n",
      "Do you have access to a shared calendar?  \n",
      "    If yes, which shared calendar?  YES, West Calendar\n",
      "\n",
      "Do you have any Distribution Groups that Messaging maintains for you (for mass mailings)?  \n",
      "    If yes, please list here:  NO\n",
      "\n",
      "Please list all Notes databases applications that you currently use:  NONE\n",
      "\n",
      "In our efforts to plan the exact date/time of your migration, we also will need to know:\n",
      "\n",
      "What are your normal work hours?  From:     7:00 AM To:      5:00 PM\n",
      "\n",
      "Will you be out of the office in the near future for vacation, leave, etc? NO\n",
      "       If so, when?        From (MM/DD/YY):      To (MM/DD/YY):      \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<Embedded StdOleLink>\n"
     ]
    }
   ],
   "source": [
    "key = keys[50]\n",
    "val = vals[50]\n",
    "\n",
    "print(key)\n",
    "\n",
    "print(hash_map.get(key))\n",
    "print(hash_table_l.get(key))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb3449f-1380-43f7-a76c-49e0815e7823",
   "metadata": {},
   "source": [
    "Let's try a couple more tests. How about we try a 'k' depth of 2 and one of 4? Theoretically, the more depth of k you have, the more precision the RMI structure has with its models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ba2085a9-8589-4554-8672-8b2c2d39d4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the enron email dataset with an RMI depth of 2. Since these keys are all unique a uniform distribution is a better approximation\n",
    "# than a normal distribution. The isText parameter is true since the keys are strings.\n",
    "hash_table_l_2 = LearnedHashMap(len(keys), keys, vals, 2, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c2c37a27-0af2-4abe-b232-55eb56226e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Hash Collisions: 460935\n",
      "Average number of collisions per key: 0.9999956610122425\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of Hash Collisions: {hash_table_l_2.get_conflicts()}')\n",
    "print(f'Average number of collisions per key: {hash_table_l_2.get_conflicts() / len(keys)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a5bdaa-ffff-4d1e-a369-46a4cbbfa25f",
   "metadata": {},
   "source": [
    "Surprisingly, we see about half the number of collisions as with a 'k' value of 3. This could be due to the fact that uniform distribution CDFs are a perfectly linear line, as such more models might actually be leading to more uncertainty which increases hash collisions unnecessarily since there isn't a lot of variation in the CDF. Regardless, let's try with a 'k' value of 4 and see if we may be correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f0133ee0-8bd8-4dc0-b149-23b7248980ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the enron email dataset with an RMI depth of 4. Since these keys are all unique a uniform distribution is a better approximation\n",
    "# than a normal distribution. The isText parameter is true since the keys are strings.\n",
    "hash_table_l_4 = LearnedHashMap(len(keys), keys, vals, 4, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b6845320-2045-4ac3-8c27-9cf8dbba818f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Hash Collisions: 1497973\n",
      "Average number of collisions per key: 3.2498432540672586\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of Hash Collisions: {hash_table_l_4.get_conflicts()}')\n",
    "print(f'Average number of collisions per key: {hash_table_l_4.get_conflicts() / len(keys)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb298b9-e317-4c0c-9486-6c1606373768",
   "metadata": {},
   "source": [
    "It appears that our earlier hypothesis may be correct with regards to the uniform CDF. An increased value of 'k' may just be leading to further uncertainty rather than flattening out variation in the CDF (since there is none). Now let's try some randomly, synthetically produced data from a normal distribution, and compare to the traditional hashmap. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "58985325-7e6e-4055-9d8c-80c37b318f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This generates 100,000 values from a normal distribution with a mean of 5,000 and a standard deviation of 10.\n",
    "keys_norm = np.random.normal(5000, 10, 100000)\n",
    "\n",
    "hash_map_norm = HashMap(len(keys_norm), keys_norm, keys_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6eca6efb-822f-4d5e-b8bb-60674631d27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Hash Collisions: 52463\n",
      "Average number of collisions per key: 0.52463\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of Hash Collisions: {hash_map_norm.get_conflicts()}')\n",
    "print(f'Average number of collisions per key: {hash_map_norm.get_conflicts() / len(keys_norm)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0d116e2a-5009-4836-8787-55192abf9f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The learned hashmap will use the keys_norm as both the key and value with a RMI depth of 3, 'isNormal' set to true since we are\n",
    "# approximating a normal distribution, and 'isText' set to false since the keys are numerical data.\n",
    "hash_table_lnorm = LearnedHashMap(len(keys_norm), keys_norm, keys_norm, 3, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "01b24eb1-7d3e-4412-adba-e5f7450dbf17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Hash Collisions: 20034373\n",
      "Average number of collisions per key: 200.34373\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of Hash Collisions: {hash_table_lnorm.get_conflicts()}')\n",
    "print(f'Average number of collisions per key: {hash_table_lnorm.get_conflicts() / len(keys_norm)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb98880-c3f4-49b8-89b1-7e82211ef0f1",
   "metadata": {},
   "source": [
    "This was actually a huge difference. Although 200 is still small enough to not be considered proportional to the input size of 100,000, considering that the average for the traditional hashmap was 0.5 collisions per key, there is a huge difference in performance. Our hypothesis after running this and doing many debugging tests is that the models in the RMINodes may be having a difficult time with the normal approximation method. There are other methods to try, but we ran into a lot of deadends and eventually just ran out of time.\n",
    "\n",
    "Finally, using the best learned hashmap with a 'k' of 2 containing the enron email dataset, we will compare its runtime to lookup 100,000 keys in random order to the same on the traditional hashmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "29cdeecf-9be3-4d61-85a3-4ae0be2c25c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "36291b50-83c5-495e-999b-2387012b5a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly permute the order of items in the keys array, this will allow for random lookups.\n",
    "random.shuffle(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "17ff686b-2093-4595-9c62-c5a87eac43df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to randomly lookup all keys with the traditional hashmap: 0.13964319229125977\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for i in range(0, 100000):\n",
    "    hash_map.get(keys[i])\n",
    "\n",
    "mid_time = time.time()\n",
    "\n",
    "for i in range(0, 10000):\n",
    "    pass\n",
    "    # Gets the overhead of the loop\n",
    "\n",
    "stop_time = time.time()\n",
    "\n",
    "print(f'Total time to randomly lookup all keys with the traditional hashmap: {(mid_time - start_time) - (stop_time - mid_time)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "82b72dce-dca5-4c3a-90fd-548dfadd0ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to randomly lookup all keys with the learned hashmap with k = 2: 5.924741268157959\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for i in range(0, 100000):\n",
    "    hash_table_l_2.get(keys[i])\n",
    "\n",
    "mid_time = time.time()\n",
    "\n",
    "for i in range(0, 10):\n",
    "    pass\n",
    "    # Gets the overhead of the loop\n",
    "\n",
    "stop_time = time.time()\n",
    "\n",
    "print(f'Total time to randomly lookup all keys with the learned hashmap with k = 2: {(mid_time - start_time) - (stop_time - mid_time)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bac279-8814-4cca-b342-688c06fbb529",
   "metadata": {},
   "source": [
    "Analyzing this runtime, it does seem that the traditional hashmap has better data distribution than our learned hashmap does. Although it isn't perfect, this was definitely a great learning experience and has lots of room for improvement in the future with different models and neural networks. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
